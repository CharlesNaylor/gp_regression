---
title: "Backtesting"
output: html_notebook
---

# Efficiency
Now that we have a validated model, and we've tested it forecasting some of our actual data, it's time to run a backtest. 

## Scoping the Problem
We know that, unless we cap the total number of weeks on which we run the regression, the model complexity is going to rise at least at $O(n^2)$, because the Gaussian Process requires us to pass an NxN matrix as the beta covariance. The good news is that we have, relatively, a tiny data set compared to what a machine learning expert would want for, say, a recursive neural net. And Stan is improving quickly, with the possibility to parallelize individual chains and use GPUs promised soon (I'm writing this in September, 2017). At the moment, I'm estimating the last week for which we have data, in which N will be 451, should take about 4 & 1/2 hours. So we are early on the inevitable quadratic complexity curve. It may be possible to keep ahead of the increasing complexity of the model due to improvements in Stan's algorithms and in the hardware on which they run. Failing that, we might take shortcuts like using the optimizer with initial values from the previous run.

To see if it's viable to use the optimizer, we will need to see how much previous betas change over time in the full generative model. I'm going to use Syracuse University's Research Computing Lab to run all the backtesting periods independently so we can see how much drift we might expect.

This means I'm going to have to prepare 400 separate data files, one for each week with exactly the right amount of data. I could probably rewrite the Stan code to do it in the `transformed data` block, but R is much easier to debug, and these files are small.

## Efficiency Tweaks


#Set up the data
```{r setup}
library(tidyverse)
library(lubridate)
library(rstan)

if(!dir.exists("../data/weekly")) {
  dir.create("../data/weekly")
}

load("../data/forecasting.rData")
```

The command line version of Stan wants data in a particular format that can be generated by `rstan::stan_rdump`. Annoyingly, stan_rdump insists on receiving a vector of character strings naming objects in the environment, rather than a list of the actual data, so I'll be making my modifications in a pocket environment rather than just assigning everything to a list. We could mess around with a new environment directly, using `new.env`, but any function creates its own environment, so let's go that route.

```{r dump_data}
dump_data <- function(N, path, A=dim(exogs)[2], min_N1=52, 
                      N2=1, B=dim(exogs)[3], y_offset=104,
                      p_endo=endo, p_exogs=exogs, p_y_cov=y_cov,
                      FIRST_DATA_SET=52, debug=F) {
  N1 <- min_N1 + N
  y <- na.omit(p_endo[(1+y_offset):(y_offset+N1),-1])
  x <- aperm(p_exogs[(1+FIRST_DATA_SET):(FIRST_DATA_SET+N1+N2),,],c(1,3,2))
  y_cov <- p_y_cov[1:(N1+N2),,]
  if(debug) {
    return(list(N1=N1,N2=N2,A=A,B=B,y=y,x=x,y_cov=y_cov))
  } else {
    rstan::stan_rdump(c("N1","N2","A","B","y","x","y_cov"), 
                    file=paste0(path,"gp_",N,".dat"))
  }
}

walk(0:399, dump_data, path="../data/weekly/")
```

#Run it elsewhere

I compiled the gp_forecast.stan using [CmdStan](http://mc-stan.org/users/interfaces/cmdstan.html) which lets you run a single chain directly from the bash shell. This means it's easy to script. Syracuse uses [Condor](https://en.wikipedia.org/wiki/HTCondor) in its computer labs, which runs jobs on idle processors. The disadvantage is that your job can be arbitrarily killed if somebody logs on. As a result, about 1450 of the 1600 jobs I needed to run (400 sets of data above times 4 chains each) finished in 8 hours, but the remainder took days. Condor has hooks for resuming a run, and really you could just start over with fewer iterations and concatenate the samples if interrupt, but Stan doesn't have an easy mechanism to make that happen. I think after the MPI stuff comes through (and that should be soon!), this would be structured quite differently.

In case you're curious, the condor job submit code looks like this:
```
executable      =/home/cnaylor/bin/gp_forecasts 
arguments       ="data file=/home/cnaylor/weekly/gp_$(Process).dat sample num_samples=500 num_warmup=500 random seed=8675$(Process) output file=samples_$(Process)_0" 
output          = ../logs/output_$(Process)_0 
error           = ../logs/error_$(Process)_0 
log             = ../logs/gp.log 
request_cpus    = 1 
Requirements    = Memory >= 100 
Image_Size      = 101000 
Rank            = kflops 
queue 400 
arguments       ="data file=/home/cnaylor/weekly/gp_$(Process).dat sample num_samples=500 num_warmup=500 random seed=8675$(Process) output file=samples_$(Process)_1" 
output          = ../logs/output_$(Process)_1 
error           = ../logs/error_$(Process)_1 
queue 400 
arguments       ="data file=/home/cnaylor/weekly/gp_$(Process).dat sample num_samples=500 num_warmup=500 random seed=8675$(Process) output file=samples_$(Process)_2" 
output          = ../logs/output_$(Process)_2 
error           = ../logs/error_$(Process)_2 
queue 400 
arguments       ="data file=/home/cnaylor/weekly/gp_$(Process).dat sample num_samples=500 num_warmup=500 random seed=8675$(Process) output file=samples_$(Process)_3" 
output          = ../logs/output_$(Process)_3 
error           = ../logs/error_$(Process)_3 
queue 400
```

If someone knows a way to loop the four chains, I'm all ears.

At the end of this process, I have 1600 files full of generated parameter samples, totaling about 13GB of data. For obvious reasons, I'm not including them in git.

Let's have a look.

#Examining CmdStan Output

I don't have 13GB of RAM on this machine, and R keeps everything in memory, so I'm going to load and process this data one fit at a time. It's good practice not to assume you'll be able to hold all results in memory anyway. `ggplot2` is a memory hog, and you do not want to be stuck moving stuff in and out of swap when you try to graph something big.

As a result, I'll do my analysis in one pass of loading data files, then look at the result later.

We need to:
  * Check model fits and convergence on at least a representative sample of chains
  * Check the stability of betas across runs
  * Assemble a complete set of weekly forecast distributions to compare to actual returns
  
## Functions
A little house-keeping first. We need to translate the numbered files to corresponding dates.
```{r load_samples}
N2dt <- function(N, return_all=F, MIN_N1=52, FIRST_DATA_SET=52,
                 N2=1, dts=as.Date(dimnames(exogs)[[1]])) {
  OFFSET <- MIN_N1+N2
  if(return_all) {
    return(dts[OFFSET:(OFFSET+FIRST_DATA_SET+N)])
  }
  else {
    return(dts[OFFSET+FIRST_DATA_SET+N])
  }
}
load_fit <- function(N, DATA_PATH="../data/backtest", 
                     dts=as.Date(dimnames(exogs)[[1]])) {
  #Since R is also 1-indexed, the index equals the number on the data files + 1
  files <- paste0(DATA_PATH,"/samples_",N,"_",0:4)
  exist <- sapply(files,file.exists)
  if(!all(exist)) {
    if(all(!exist)) {
      message(paste("No samples for",N))
      return(NULL)
    }
    nc <- nchar(files[1]) #filenames will always be same length
    #warning(paste0("Loading files for ", N, 
    #               ", but missing chains: ", 
    #               paste(substr(files[!exist],nc,nc), 
    #                    collapse=", ")))
    files <- files[exist]
  }
  fit <- tryCatch( {
    fit <- rstan::read_stan_csv(files)
    fit@model_name <- format(N2dt(N, dts=dts), "%Y-%m-%d")
    return(fit)
  },
  error=function(cond) {
    message(paste(N, "failed.",cond))
    return(NULL)
  }
  )
}
```

### Check Fits

CmdStan has generated a large number of log files, which I can search for common error messages like 'divergent transitions', or 'max_treedepth reached'. Just use the classic find -> grep pattern:
`find ./logs/* -exec grep divergent {} \;`, and see if anything comes up. Nothing did for me. You could potentially have divergences if you got unlucky with the seed. This is generally not likely, but certainly a possibility with GPs.

The other thing to check is that all of the chains actually ran to completion. Recall this is not a guarantee with the HTCondor setup.
`find ./data/backtest/ -iname "samples*" -exec grep -LH terminated {} \; > not_terminated` reveals for me 9 stragglers, which isn't bad out of 1600 total runs. I reran these guys on dedicated hardware. I could also have grepped 'Abnormal Termination' on the log files.

For convergence, our main problems will be the length_scale and the beta correlation matrix. From what I can find in the literature, there's no canonical way to check convergence of chains. Usually I would eyeball them, but here we have too many runs.

```{r fun_convergence}
#measure overlap between chains
check_convergence <- function(fit) {
  #Check Sum of squared deviations from mean of sd-adjusted quartiles
  # SDs of zero mean stuck chains.
  csum <- summary(fit)$c_summary
  stats <- c("sd","25%","75%")
  
  if(any(csum[,"sd",]==0)) {
    #Stuck chains
    return("stuck")
  }
  conv <- apply(csum[,stats,],1,function(X){
    x <- matrix(X,nrow=3)
    x <- x[2:3,]/x[1,]
    return(sum((x-mean(x))^2))
    })
  nas <- names(conv)[is.na(conv)]
  if(!all(grepl("^L_",nas))) { 
    message(paste0("Convergence NAs in ",fit@model_name,": ", 
                   paste0(nas[!grepl("^L_",nas)])))
  }
  return(TRUE %in% (na.omit(conv) > 50))
}
```

### Assemble Betas

I will be keeping only point estimates of historical betas by run. If I kept the full distribution, we would have too much to look at. The exception is the last row of betas in each of these runs, which are forecasts. I want to keep those distributions so I can see how much of the endogenous forecast variance is due to variance in the betas, and how much to the given covariance of `y`.

```{r fun_betas}
get_betas <- function(fit, N, dts=N2dt(N,T),
                      beta_names=dimnames(exogs)[[3]],
                      n_iter=500) {
  betas <- extract(fit, pars="beta")[["beta"]]
  median_beta <- apply(betas,2:3,median)
  dimnames(median_beta) <- list(Date=format(dts,"%Y-%m-%d"),beta=beta_names)
  forecast_beta <- betas[,dim(betas)[2],]
  dimnames(forecast_beta) <- list(iterations=NULL,beta=beta_names)
  return(list(median=median_beta,forecast=forecast_beta))
}
```

### Assemble Forecast distributions

I want the full distribution for forecasts, too.
```{r fun_yhat}
get_yhat <- function(fit, y_names=dimnames(exogs)[[2]]) {
  ys <- extract(fit, pars="new_y")[["new_y"]][,1,]  #The stan code is written so you can forecast beyond one week, but we didn't do that
  colnames(ys) <- y_names
  return(ys)
}
```

## Run the functions

Now we need to iterate over these functions. I'm going to pre-allocate data structures since we know exactly how each should be shaped. I think an old-fashioned loop is clearer than `lapply` or the `purrr` functions here, as we're doing four things at once. If I didn't want to minimize disk access, I'd use separate `lapply`s for each variable (or call `purrr:map`).
```{r assemble_data}
N <- 400
betas <- vector('list', N)
all_dts <- N2dt((N-1), T)
names(betas) <- format(all_dts[53:452], "%Y-%m-%d")

y_names=dimnames(exogs)[[2]]
beta_names=dimnames(exogs)[[3]]
n_iter=1000
length_scales =array(NA,dim=c(1000,N,6))

forecast_betas <- array(dim=c(n_iter, N, length(beta_names)),
                        dimnames=list(iterations=NULL, Date=names(betas), 
                                      beta=beta_names))
yhat <- array(dim=c(n_iter, N, length(y_names)),
              dimnames=list(iterations=NULL, Date=names(betas), 
                            asset=y_names))

convergence_problems <- vector('numeric',N)
names(convergence_problems) <- names(betas)

for(i in 0:(N-1)) {
  if(i %% 50 == 0) {
    message(i)
  }
  dt <- N2dt(i, dts=all_dts)
  fit <- load_fit(i, dts=all_dts)
  if(!is.null(fit)) {
    convergence_problems[i+1] = check_convergence(fit)
    
    p_betas <- get_betas(fit, i, beta_names=beta_names)
    betas[[format(dt,"%Y-%m-%d")]] <- p_betas$median
    forecast_betas[,i+1,] <- p_betas$forecast
    
    yhat[,i+1,] <- get_yhat(fit, y_names=y_names)
    length_scales[,i+1,] <- extract(fit,pars=c("length_scale"))[["length_scale"]]
  }
  else {
    message(paste(i,"failed"))
  }
}

```

3 of these failed, but had 'Adaption terminated' in the samples. Looks like I'd be better off checking if the sample file has "Elapsed" at the end.

## Results

### Assemble Fitted Betas

```{r graph_betas}

```


