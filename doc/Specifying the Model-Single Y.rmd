---
title: "Specifying the Model, part 1- Validate Single Y"
output:
  html_document: 
    fig_width: 8.05
    toc: yes
  html_notebook: default
---

# Model Overview

This is following [Jim Savage's Bayesian Workflow](http://nbviewer.jupyter.org/github/QuantEcon/QuantEcon.notebooks/blob/master/IntroToStan_basics_workflow.ipynb)

We are going to run a dynamic linear regression, in which inputs X relate to asset returns Y on the basis of betas B, which vary over time as a Gaussian process. Gaussian processes are discussed at length in chapter 18 of the Stan reference manual.

In full:
$$
  y_{t+1} \sim N(\beta_t X_t, \Sigma_y) \\
  \beta_d \sim MN(0, \Sigma_{N,d}, \Sigma_{\beta}) \\
  \Sigma(N, d)_{i,j} = \alpha_d^2exp\left (-\frac{1}{2\rho_d^2}(i-j)^2 \right ) + \delta_{i,j}\\
  \rho_d \sim \gamma^{-1}(5,5) \\
  \begin{matrix}\alpha_d \sim N(0, 1) & \alpha > 0 \end{matrix} \\
  \delta_{i,j} \left\{\begin{matrix}
 \sim N(0,1) & i \equiv j\\
 0 & i \neq j
\end{matrix} \right{} \\ \\
\Sigma_\beta = \Omega'\tau\Omega \\
\Omega \sim LJKCorr(3) \\
\tau \sim cauchy(3), \tau > 0
$$

In plain English:

1) next period's Y depends on this period's inputs X, weighted by this period's betas $\beta$. Ys move together across assets with covariance $\Sigma_y$. N, here, is the multivariate-normal distribution.

2) Each $\beta_d$ is distributed matrix-variate-normal, with $\mu$ = 0. The $\beta_d$ is correlated with itself over all times $t \in {1 \dotsi T}$ with covariance matrix $\Sigma_{N, d}$. Additionally, all $\beta$s covary with each other according to $\Sigma_\beta$. We include this term because the exogenous variables are not independent.

3) Each $\Sigma_{N, d}$ is a Gaussian process, in which the degree of similarities between elements varies according to their squared Euclidian distance from one another (cf. Stan Reference Manual 2.17.0, pp.246-247). As the length of each measurement period is constant, the distance between row $i$ and row $j$ is simply the difference between $i$ and $j$.

4) $\rho$ and $\alpha$ are parameters defining the speed at which $\Sigma_{N,\beta}$ terms become unrelated to each other. $\delta$ is the scale of the noise term in the regression, and guarantees a positive-definite matrix. Betas are not independent of each other, but their parameters $\alpha$ and $\rho$ *are*. 

5) $\Sigma_\beta$ is the covariance matrix between betas. My exogenous variables are correlated. For example, I have both the relative level of 2Y rates, and the relative level of *changes* in 2Y rates. We account for this with two additional parameters, $\Omega$, representing the correlation, and $\tau$, representing the variance. $\tau$ is going to interact with $\Sigma_{N, \beta} as a whole, so we'll need to do some testing to decide what the prior distribution should look like. The ratio between the two is going to determine whether covariation or variation over time dominate, and we want the variation over time to be more important. I'm putting a half-cauchy distribution in for now, as that's the prior the mc-stan guys recommend, and it is weighted closer to 0 than, say, the Gaussian distribution.

We are going to start with a single-asset, single beta parameter implementation of that model. We'll generate beta time series for each column of X, but use only one $\rho$ to define the whole set.

# Gaussian Process Regression for a Single Asset, single factor

```{r setup}
knitr::opts_chunk$set(warning = FALSE, message = FALSE)
library(tidyverse)
library(rstan) #2.17 hasn't been released yet for rstan, but we'll be using cmdstan 2.17.

rstan_options(auto_write = TRUE)
options(mc.cores = parallel::detectCores())

load("../data/calculating_factors.rData")
```

**NB** RStudio actually provides a `stan` chunk-style, which will compile the model when executed. However, in my experience RStudio gets unstable when one runs a large stan model inside it. Additionally, the chunks don't show up when you compile notebooks to html with knitr. I'm just going to state the model here. This is [stan/single_y_gp.stan](https://github.com/billWalker/gp_regression/blob/master/stan/single_y_gp.stan) in the repository.
```{r}
cat(read_file("../stan/single_y_gp.stan"))
```

Before we implement that model on actual data, let's follow the workflow I linked at the top and see if it can recover a set of known parameters.

## Generate Fake Data

```{r secret_load, include=FALSE}
#Load from saved data as I don't want to have to run these 90-minute generative processes every time I call knitr to html.
#This file is too big for github, hence the 'full' qualifier. Stan objects are big.
load("../data/specifying_the_model-single_y-full.rData")
```

We'll need to write a function in stan that can simulate results when given the parameters. The model I stated above can simulate Ys (and the distribution of those simulated Ys gives you the forecast distribution), but the code has $\beta$, $\alpha$, and $\sigma$ as factors. We want to specify $\alpha$ and $\sigma$, generate fake data on the basis of that specification, then apply the model above to see if it can recover them.

```{r stan_sim, eval=F}
sim_model <- stan_model(model_name='single_gp_sim',
                   model_code=read_file("../stan/single_y_gp_sim.stan"))
```

Normally I'd write a function and use `rstan::expose_stan_functions`, but I'm having a cpp error I can't debug, so we have this model instead. It's the same model as above, except I've switched the parameters and data roles for y and the parameters we want to recover.

```{r make_fake_data, eval=F}
set.seed(8675309) #Let's keep the randomness consistent
fake_data = list(N=100, D=3, alpha=c(0.1, 0.25, 0.5), rho=1.0, sigma=0.05)
fake_data <- within(fake_data, {
  x <- matrix(rnorm(N*D), nrow=N, ncol=D)
  eta <- matrix(NA,nrow=N,ncol=D)
  eta[1,] <- 0
  for(i in 2:N) {
    eta[i,] <- eta[i-1,]*0.99 + rnorm(D, 0.01, 0.1)
  }
  Omega <- matrix(0,nrow=D,ncol=D)
  diag(Omega) <- 1
  Omega[lower.tri(Omega)] <- runif(D,min=-1,max=1)
  Omega <- crossprod(Omega)
  diag(Omega) <- 1
})
```

```{r graph_fake_data}
fake_data$eta %>% as_tibble() %>% mutate(t=seq.int(100)) %>% 
  gather(beta,value, -t) %>% ggplot(aes(x=t,y=value,col=beta)) +
  geom_line() + ggtitle("Fake betas over time")
```
```{r graph_fake_XB}
with(fake_data,x*eta) %>% as_tibble() %>% mutate(t=seq.int(100)) %>% 
  gather(beta,value, -t) %>% ggplot(aes(x=t,y=value,col=beta)) +
  geom_line() + ggtitle("Fake XBs over time")
```

It's not all that likely that we'll be able to get close to these betas with only 100 data points, but I'd hope at least that we'll have something in range on $\alpha$, $\rho$, and $\sigma$.
```{r fake_endo, eval=F}
sim_samples <- sampling(sim_model, fake_data, cores=2, chains=2)
```
Now we have 4000 possible series based on those parameters. Let's see how different they are. We're going to wind up using the median to test recovering parameters.
```{r graph_sim_y}
sim_y <- extract(sim_samples)$y
sim_y %>% t() %>% as_tibble() %>%
  select(num_range("V",sample.int(4000, 100))) %>%
  mutate(Date=seq.int(100)) %>% gather(sample,value,-Date) %>%
  ggplot(aes(x=Date,y=value, group=sample)) + geom_line(alpha=0.05) + 
  ggtitle("Y sims")
```

This is wildly optimistic. Relatively speaking, the real Y probably has a $\sigma$ an order of magnitude higher, as the noise overwhelms the signal. Keep in mind, this is meant to be a stochastic time series, not a random walk. Anyway, let's see if we can recover that.

## Recover Known Parameters

```{r recover_params, eval=F}
single_gp_model <- stan_model(file="../stan/single_y_gp.stan", model_name='single_y_gp')
single_fit <- sampling(single_gp_model, 
                       within(fake_data, {y<-apply(sim_y,2,median)})[c("N","D","x","y")])
```

Amazingly, I had no divergent transitions on this one. This sample took 90 minutes to run on my laptop's i7, 2.3GHz, thanks to a stuck chain. Would have been 45 minutes otherwise. Stan is supposed to be adding GPU support for Gaussian processes very soon. That's going to make this a lot more practical on consumer-grade hardware, as will the addition of `cov_exp_quad_cholesky`. Probably, there's an optimization that could be performed to this kernel given that the distance term is always an integer, but I don't know it.

Anyway, let's see if we came reasonably close to recovering the true parameters.

```{r, message=FALSE, warning=FALSE}
stopifnot(require(bayesplot))

mcmc_intervals(as.array(single_fit), pars=c(paste0("alpha[",1:3,"]"), "rho", "sigma"))
```

$\rho$ came in on target, but and $\sigma$ is a little low, and two of the $\alpha$s are out of range. Per the manual, $\alpha$ is the marginal standard deviation of the covariance function, and is recoverable by taking the standard deviation of a set of functions generated by `cov_exp_quad` at a fixed $\alpha$. We didn't save those, but we do have 2000 endogenous time series drawn from the fixed $\alpha$, whose median we used to fit the above parameters. The standard deviation of those series is
```{r sd_y_sim}
sd(sim_y)
```

Which is very close to the average of the three $\alpha$s. The maximum fitted $\alpha$s are very low:
```{r max_alpha}
apply(extract(single_fit)$alpha,2,max)
```

Were the chains well-mixed?
```{r, fig.height=10, fig.width=16.1}
mcmc_trace(as.array(single_fit), pars=c(paste0("alpha[",1:3,"]"), "rho", "sigma"))
```
So-so for $\sigma$.

Let's see how things look in the multivariate endo case.

```{r save_data, eval=F}
save(endo, exogs, sim_y, single_fit, file="../data/specifying_the_model-single_y.rData")
```

