---
title: "Specifying the Model, part 1- Validate Single Y"
output:
  html_document: 
    toc: yes
  html_notebook: default
---

# Model Overview

This is following [Jim Savage's Bayesian Workflow](http://nbviewer.jupyter.org/github/QuantEcon/QuantEcon.notebooks/blob/master/IntroToStan_basics_workflow.ipynb)

We are going to run a dynamic linear regression, in which inputs X relate to asset returns Y on the basis of betas B, which vary over time as a Gaussian process. Gaussian processes are discussed at length in chapter 18 of the Stan reference manual.

In full:
$$
  y_{t+1} \sim MN(\beta_t X_t, \Sigma_y) \\
  \beta_i \sim MN(0, \Sigma_{\beta_i}) \\
  \Sigma(\beta)_{i,j} = \alpha^2exp\left (-\frac{1}{2\rho^2}\sum_{d=1}^D(x_{i,d}-x_{j,d})^2 \right ) + \sigma^2\\
  \rho \sim \gamma^{-1}(5,5) \\
  \begin{matrix}\alpha \sim N(0, 1) & \alpha > 0 \end{matrix} \\
  \sigma_{i,j} \left\{\begin{matrix}
 \sim N(0,1) & i \equiv j\\
 0 & i \neq j
\end{matrix}\right.
$$

I'll try to go through the whole thing in plain english:

1) next period's Y depends on this period's inputs X, weighted by this period's betas $\beta$. Y's move together with covariance $\Sigma_y$. MN stands for the multivariate-normal distribution.

2) Each $\beta_i$ is distributed multivariate-normal, with $\mu$ = 0. The $\beta_i$ is correlated with itself over all times $t \in {1 \dotsi T}$ with covariance matrix $\Sigma_{\beta_i}$.

3) Each $\Sigma_{\beta_i}$ is a Gaussian process, in which the degree of similarities between elements varies according to their squared Euclidian distance from one another (cf. Stan Reference Manual 2.17.0, pp.246-247).

4) $\rho$ and $\alpha$ are parameters defining the speed at which $\Sigma_{\beta}$ terms become unrelated to each other. $\sigma^2$ is the scale of the noise term in the regression.


**NB**:
* We are assuming that betas are independent of each other, *which is wrong*. There are just too many parameters for my laptop to have a hope of entangling the Gaussian processes. Rob Trangucci has an [example](https://github.com/stan-dev/example-models/blob/master/misc/gaussian-process/gp-fit-multi-output.stan) of a Gaussian process with a single input parameter and multiple, correlated outputs.

We are going to start with a single-asset implementation of that model. 

# Gaussian Process Regression for a Single Asset

```{r setup}
knitr::opts_chunk$set(warning = FALSE, message = FALSE)
library(tidyverse)
library(rstan) #2.17 hasn't been released yet for rstan, but we'll be using cmdstan 2.17.

rstan_options(auto_write = TRUE)
options(mc.cores = parallel::detectCores())

load("../data/calculating_factors.rData")
```

**NB** RStudio actually provides a `stan` chunk-style, which will compile the model when executed. However, in my experience RStudio gets unstable when one runs a large stan model inside it. Additionally, the chunks don't show up when you compile notebooks to html with knitr. I'm just going to state the model here. This is [stan/single_y_gp.stan](https://github.com/billWalker/gp_regression/blob/master/stan/single_y_gp.stan) in the repository.
```{r}
cat(read_file("../stan/single_y_gp.stan"))
```

Before we implement that model on actual data, let's follow the workflow I linked at the top and see if it can recover a set of known parameters.

## Generate Fake Data

```{r secret_load, include=FALSE}
#Load from saved data as I don't want to have to run these 90-minute generative processes every time I call knitr to html.
load("../data/specifying_the_model-single_y.rData")
```

We'll need to write a function in stan that can simulate results when given the parameters. The model I stated above can simulate Ys (and the distribution of those simulated Ys gives you the forecast distribution), but the code has $\beta$, $\alpha$, and $\sigma$ as factors. We want to specify $\alpha$ and $\sigma$, generate fake data on the basis of that specification, then apply the model above to see if it can recover them.

```{r stan_sim, eval=F}
sim_model <- stan_model(model_name='single_gp_sim',
                   model_code=" // multi-dimensional input latent variable GP with single y
// cf. manual 2.17.0, pp 253-254
data {
  int<lower=1> N;
  int<lower=1> D;
  vector[D] x[N]; //Exogs for single currency
  real<lower=0> rho;
  real<lower=0> alpha;
  real<lower=0> sigma;
  matrix[N, D] eta;
}
transformed data {
  real delta = 1e-9;
  vector[D] ones = rep_vector(1,D);
}
parameters {
  vector[N] y;
}
model {
  vector[N] f;
  matrix[N, N] K = cov_exp_quad(x, alpha, rho);
  matrix[N, N] L_K;
  //perturb diagonal elements
  for (n in 1:N)
    K[n, n] = K[n, n] + delta;
  L_K = cholesky_decompose(K);
  f = (L_K * eta) * ones; //post-multiply by a row of ones to sum by row

  y ~ normal(f, sigma);
}
")
```

Normally I'd write a function and use `rstan::expose_stan_functions`, but I'm having a cpp error I can't debug, so we have this model instead. It's the same model as above, except I've switched the parameters and data roles for y and the parameters we want to recover.

```{r make_fake_data, eval=F}
set.seed(8675309) #Let's keep the randomness consistent
fake_data = list(N=100, D=3, alpha=0.25, rho=1.0, sigma=0.01)
fake_data <- within(fake_data, {
  x <- matrix(rnorm(N*D), nrow=N, ncol=D)
  eta <- matrix(NA,nrow=N,ncol=D)
  eta[1,] <- 0
  for(i in 2:N) {
    eta[i,] <- eta[i-1,] + rnorm(D, 0, 0.05)
  }
})
```

```{r graph_fake_data}
fake_data$eta %>% as_tibble() %>% mutate(t=seq.int(100)) %>% 
  gather(beta,value, -t) %>% ggplot(aes(x=t,y=value,col=beta)) +
  geom_line() + ggtitle("Fake betas over time")
```

It's not all that likely that we'll be able to get close to these betas with only 100 data points, but I'd hope at least that we'll have something in range on $\alpha$, $\rho$, and $\sigma$.
```{r fake_endo, eval=F}
sim_samples <- sampling(sim_model, fake_data, cores=2)
```
Now we have 4000 possible series based on those parameters. Let's see how different they are. We're going to wind up using the median to test recovering parameters.
```{r graph_sim_y}
sim_y <- extract(sim_samples)$y
sim_y %>% t() %>% as_tibble() %>%
  select(num_range("V",sample.int(4000, 100))) %>%
  mutate(Date=seq.int(100)) %>% gather(sample,value,-Date) %>%
  ggplot(aes(x=Date,y=value, group=sample)) + geom_line(alpha=0.05) + 
  ggtitle("Y sims")
```

Probably should have picked a higher $\sigma$. Anyway, let's see if we can recover that.

## Recover Known Parameters

```{r recover_params, eval=F}
single_gp_model <- stan_model(file="stan/single_y_gp.stan", model_name='single_gp')
single_fit <- sampling(single_gp_model, 
                       within(fake_data, {y<-apply(sim_y,2,median)})[c("N","D","x","y")])
```

Divergent transitions are a big problem when fitting Gaussian processes. Michael Betancourt will tell you that any number of divergent transitions means the model may be mis-specified. However, to my knowledge, you just can't avoid them here. We are fitting a lot of parameters. 

This sample took 90 minutes to run on my laptop's i7, 2.3GHz. Stan is supposed to be adding GPU support for Gaussian processes very soon. That's going to make this a lot more practical on lesser hardware, as will the addition of `cov_exp_quad_cholesky`.

Anyway, let's see if we came reasonably close to recovering the true parameters.

```{r, message=FALSE, warning=FALSE}
stopifnot(require(bayesplot))

mcmc_intervals(as.array(single_fit), pars=c("alpha", "rho", "sigma"))
```

$\rho$ and $\sigma$ came in on target, but $\alpha$ is a little low. I'm tempted to say that $\sqrt{\alpha}$ is on target, and we know the term is squared in `cov_exp_quad`, but I don't think that's it. Per the manual, $\alpha$ is the marginal standard deviation of the covariance function, and is recoverable by taking the standard deviation of a set of functions generated by `cov_exp_quad` at a fixed $\alpha$. We didn't save those, but we do have 4000 endogenous time series drawn from the fixed $\alpha$, whose median we used to fit the above parameters. The standard deviation of those series is...
```{r sd_y_sim}
require(knitr)
kable(tibble(`SD of sim_y`=sd(sim_y),  `median fitted alpha`= median(extract(single_fit)$alpha)),
      digits=4, align='c', format='latex')
```

So maybe the difference comes from my generating process, not the fit on the simulated data.