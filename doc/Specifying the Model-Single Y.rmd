---
title: "Specifying the Model, part 1- Validate Single Y"
output:
  html_document: 
    fig_width: 8.05
    toc: yes
  html_notebook: default
---

# Model Overview

This is following [Jim Savage's Bayesian Workflow](http://nbviewer.jupyter.org/github/QuantEcon/QuantEcon.notebooks/blob/master/IntroToStan_basics_workflow.ipynb)

We are going to run a dynamic linear regression, in which inputs X relate to asset returns Y on the basis of betas B, which vary over time as a Gaussian process. Gaussian processes are discussed at length in chapter 18 of the Stan reference manual.

In full:
$$
  y_{t+1} \sim MN(\beta_t X_t, \Sigma_y) \\
  \beta_d \sim MN(0, \Sigma_{\beta_d}) \\
  \Sigma(\beta_d)_{i,j} = \alpha_d^2exp\left (-\frac{1}{2\rho_d^2}(i-j)^2 \right ) + \delta_{i,j}\\
  \rho \sim \gamma^{-1}(5,5) \\
  \begin{matrix}\alpha \sim N(0, 1) & \alpha > 0 \end{matrix} \\
  \delta_{i,j} \left\{\begin{matrix}
 \sim N(0,1) & i \equiv j\\
 0 & i \neq j
\end{matrix}\right.
$$

I'll try to go through the whole thing in plain english:

1) next period's Y depends on this period's inputs X, weighted by this period's betas $\beta$. Ys move together with covariance $\Sigma_y$. MN stands for the multivariate-normal distribution.

2) Each $\beta_d$ is distributed multivariate-normal, with $\mu$ = 0. The $\beta_d$ is correlated with itself over all times $t \in {1 \dotsi T}$ with covariance matrix $\Sigma_{\beta_d}$.

3) Each $\Sigma_{\beta_d}$ is a Gaussian process, in which the degree of similarities between elements varies according to their squared Euclidian distance from one another (cf. Stan Reference Manual 2.17.0, pp.246-247). As the length of each measurement period is constant, the distance between row $i$ and row $j$ is simply the difference between $i$ and $j$.

4) $\rho$ and $\alpha$ are parameters defining the speed at which $\Sigma_{\beta}$ terms become unrelated to each other. $\delta$ is the scale of the noise term in the regression, and guarantees a positive-definite matrix. Betas are not independent of each other, but their parameters $\alpha$ and $\rho$ *are*. 

If we wanted to, we could make the beta parameters hiearchical with respect to assets. This means we would have a global $\rho_d$ for each $\Beta_d$, but also a local $\rho_{d,a}$ and $\Beta_{d,a}$. This would be more parsimonious than generating separate distributions for $\Beta_d$ directly by asset. But I still think we have enough parameters to be going on with at the moment. We do not have an enormous amount of data.

We are going to start with a single-asset, single beta parameter implementation of that model. We'll generate beta time series for each column of X, but use only one $\rho$ to define the whole set.

# Gaussian Process Regression for a Single Asset

```{r setup}
knitr::opts_chunk$set(warning = FALSE, message = FALSE)
library(tidyverse)
library(rstan) #2.17 hasn't been released yet for rstan, but we'll be using cmdstan 2.17.

rstan_options(auto_write = TRUE)
options(mc.cores = parallel::detectCores())

load("../data/calculating_factors.rData")
```

**NB** RStudio actually provides a `stan` chunk-style, which will compile the model when executed. However, in my experience RStudio gets unstable when one runs a large stan model inside it. Additionally, the chunks don't show up when you compile notebooks to html with knitr. I'm just going to state the model here. This is [stan/single_y_gp.stan](https://github.com/billWalker/gp_regression/blob/master/stan/single_y_gp.stan) in the repository.
```{r}
cat(read_file("../stan/single_y_gp.stan"))
```

Before we implement that model on actual data, let's follow the workflow I linked at the top and see if it can recover a set of known parameters.

## Generate Fake Data

```{r secret_load, include=FALSE}
#Load from saved data as I don't want to have to run these 90-minute generative processes every time I call knitr to html.
#This file is too big for github, hence the 'full' qualifier. Stan objects are big.
load("../data/specifying_the_model-single_y-full.rData")
```

We'll need to write a function in stan that can simulate results when given the parameters. The model I stated above can simulate Ys (and the distribution of those simulated Ys gives you the forecast distribution), but the code has $\beta$, $\alpha$, and $\sigma$ as factors. We want to specify $\alpha$ and $\sigma$, generate fake data on the basis of that specification, then apply the model above to see if it can recover them.

```{r stan_sim, eval=F}
sim_model <- stan_model(model_name='single_gp_sim',
                   model_code=read_file("../stan/single_y_gp_sim.stan"))
```

Normally I'd write a function and use `rstan::expose_stan_functions`, but I'm having a cpp error I can't debug, so we have this model instead. It's the same model as above, except I've switched the parameters and data roles for y and the parameters we want to recover.

```{r make_fake_data, eval=F}
set.seed(8675309) #Let's keep the randomness consistent
fake_data = list(N=100, D=3, alpha=c(0.1, 0.25, 0.5), rho=1.0, sigma=0.05)
fake_data <- within(fake_data, {
  x <- matrix(rnorm(N*D), nrow=N, ncol=D)
  eta <- matrix(NA,nrow=N,ncol=D)
  eta[1,] <- 0
  for(i in 2:N) {
    eta[i,] <- eta[i-1,]*0.99 + rnorm(D, 0.01, 0.1)
  }
  Omega <- matrix(0,nrow=D,ncol=D)
  diag(Omega) <- 1
  Omega[lower.tri(Omega)] <- runif(D,min=-1,max=1)
  Omega <- crossprod(Omega)
  diag(Omega) <- 1
})
```

```{r graph_fake_data}
fake_data$eta %>% as_tibble() %>% mutate(t=seq.int(100)) %>% 
  gather(beta,value, -t) %>% ggplot(aes(x=t,y=value,col=beta)) +
  geom_line() + ggtitle("Fake betas over time")
```
```{r graph_fake_XB}
with(fake_data,x*eta) %>% as_tibble() %>% mutate(t=seq.int(100)) %>% 
  gather(beta,value, -t) %>% ggplot(aes(x=t,y=value,col=beta)) +
  geom_line() + ggtitle("Fake XBs over time")
```

It's not all that likely that we'll be able to get close to these betas with only 100 data points, but I'd hope at least that we'll have something in range on $\alpha$, $\rho$, and $\sigma$.
```{r fake_endo, eval=F}
sim_samples <- sampling(sim_model, fake_data, cores=2, chains=2)
```
Now we have 4000 possible series based on those parameters. Let's see how different they are. We're going to wind up using the median to test recovering parameters.
```{r graph_sim_y}
sim_y <- extract(sim_samples)$y
sim_y %>% t() %>% as_tibble() %>%
  select(num_range("V",sample.int(4000, 100))) %>%
  mutate(Date=seq.int(100)) %>% gather(sample,value,-Date) %>%
  ggplot(aes(x=Date,y=value, group=sample)) + geom_line(alpha=0.05) + 
  ggtitle("Y sims")
```

This is wildly optimistic. Relatively speaking, the real Y probably has a $\sigma$ an order of magnitude higher, as the noise overwhelms the signal. Keep in mind, this is meant to be a stochastic time series, not a random walk. Anyway, let's see if we can recover that.

## Recover Known Parameters

```{r recover_params, eval=F}
single_gp_model <- stan_model(file="../stan/single_y_gp.stan", model_name='single_y_gp')
single_fit <- sampling(single_gp_model, 
                       within(fake_data, {y<-apply(sim_y,2,median)})[c("N","D","x","y")])
```

Amazingly, I had no divergent transitions on this one. This sample took 90 minutes to run on my laptop's i7, 2.3GHz, thanks to a stuck chain. Would have been 45 minutes otherwise. Stan is supposed to be adding GPU support for Gaussian processes very soon. That's going to make this a lot more practical on consumer-grade hardware, as will the addition of `cov_exp_quad_cholesky`. Probably, there's an optimization that could be performed to this kernel given that the distance term is always an integer, but I don't know it.

Anyway, let's see if we came reasonably close to recovering the true parameters.

```{r, message=FALSE, warning=FALSE}
stopifnot(require(bayesplot))

mcmc_intervals(as.array(single_fit), pars=c(paste0("alpha[",1:3,"]"), "rho", "sigma"))
```

$\rho$ came in on target, but and $\sigma$ is a little low, and two of the $\alpha$s are out of range. Per the manual, $\alpha$ is the marginal standard deviation of the covariance function, and is recoverable by taking the standard deviation of a set of functions generated by `cov_exp_quad` at a fixed $\alpha$. We didn't save those, but we do have 2000 endogenous time series drawn from the fixed $\alpha$, whose median we used to fit the above parameters. The standard deviation of those series is
```{r sd_y_sim}
sd(sim_y)
```

Which is very close to the average of the three $\alpha$s. The maximum fitted $\alpha$s are very low:
```{r max_alpha}
apply(extract(single_fit)$alpha,2,max)
```

Were the chains well-mixed?
```{r, fig.height=10, fig.width=16.1}
mcmc_trace(as.array(single_fit), pars=c(paste0("alpha[",1:3,"]"), "rho", "sigma"))
```
So-so for $\sigma$.

Let's see how things look in the multivariate endo case.

```{r save_data, eval=F}
save(endo, exogs, sim_y, single_fit, file="../data/specifying_the_model-single_y.rData")
```

